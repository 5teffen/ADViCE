# AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation

Rapid improvements in the performance of machine learning models has pushed them to the forefront of data-driven decision-making. However, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, over-fitting, and incorrect correlations, model developers require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce *AdViCE*, a visual analytics tool that aims to guide users in black-box model debugging and validation. The solution rests on two main visual user interface innovations: (1) an interactive visualization design that enables the comparison of decisions on user-defined data subsets; (2) an algorithm and visual design to compute and visualize counterfactual explanations - explanations that depict model outcomes when data features are perturbed from their original values. Furthermore, we provide an evaluation of the work through a number of use cases that demonstrate the capabilities and potential limitations of the proposed approach.
